{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASK RCNN - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lib.mrcnn.model as modellib\n",
    "from lib.mrcnn import visualize, utils\n",
    "import lib.mrcnn.utils as mrcnn_utils\n",
    "from common.utils import get_data_dir, get_model_dir, gpu_ram_growth_config\n",
    "import logging\n",
    "from common.log import init_logging\n",
    "from service_modelTraining.maskrcnn_training import get_dataset, InferenceConfig\n",
    "#----------\n",
    "init_logging('maskrcnn_evaluate')\n",
    "MODEL_DIR = get_model_dir()\n",
    "gpu_ram_growth_config(0)\n",
    "\n",
    "MODEL_VER = 'maskrcnn_20220901_best.h5'\n",
    "model_path = os.path.join(MODEL_DIR, MODEL_VER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bunch = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=inference_config, model_dir=MODEL_DIR)\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Ground Truth\n",
    "image_id = 7\n",
    "original_image, _, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(data_bunch.val, inference_config, image_id)\n",
    "\n",
    "#Inference\n",
    "detect_ret = model.detect([original_image], verbose=0)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(121)\n",
    "r = detect_ret[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            data_bunch.val.class_names, r['scores'], figsize=(15, 15), ax=ax, title='Inference')\n",
    "ax = plt.subplot(122)\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            data_bunch.val.class_names, figsize=(15, 15), ax=ax, title='Ground Truth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPS Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_list = []\n",
    "for image_id in tqdm(data_bunch.val.image_ids):\n",
    "    img = data_bunch.val.load_image(image_id)\n",
    "    fstart = time.time()\n",
    "    results = model.detect([img], verbose=0)\n",
    "    end = time.time()\n",
    "    fps = np.round(1 / (time.time() - fstart),2)\n",
    "    fps_list.append(fps)\n",
    "fps_mean = np.mean(fps_list)\n",
    "print(f'fps: {fps_mean:.2f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "average_precision_list = []\n",
    "for image_id in tqdm(data_bunch.val.image_ids):\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(data_bunch.val, inference_config,\n",
    "                               image_id)\n",
    "    detect_ret = model.detect([image], verbose=0)\n",
    "    r = detect_ret[0]\n",
    "    # Compute AP\n",
    "    ap, precisions, recalls, overlaps =\\\n",
    "        mrcnn_utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    average_precision_list.append(ap)\n",
    "mAP = np.nanmean(average_precision_list)\n",
    "print(f'mAP: {mAP:.2f}', )\n",
    "display(pd.DataFrame({'image_id':data_bunch.val.image_ids,'ap':average_precision_list}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
